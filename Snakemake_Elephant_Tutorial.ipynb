{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snakemake Introduction to Elephant\n",
    "## Workflow management based on a electrophysiology example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table  bgcolor=\"#000000\"><tr>\n",
    "<td><img src=logos/snakemake_logo.svg alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<td><img src=http://neo.readthedocs.org/en/latest/_images/neologo.png alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<td><img src=http://elephant.readthedocs.org/en/latest/_static/elephant_logo_sidebar.png alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial I will first introduce the workflow management system [snakemake](https://snakemake.readthedocs.io) and show how this can be used for analysis of electrophysiology data using the electrophysiology analysis toolkit [Elephant](http://neuralensemble.org/elephant). For snakemake as well as Elephant tutorials exist already as part of the documentation and were partially reused in the tutorial presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: When running this tutorial locally, please first install the [requirements](environment.yml) to ensure the examples are working.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow management - Do I need this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a number of reasons for managing your managing workflows besides the classical 'I always run first script a and then script b':\n",
    "* **Growing Complexity** When starting a seemingly small and simple project everything is still pretty easy, but typically projects tend to grow beyond the initially expected size and complexity. This affects two different aspects 1) the dependency between different steps in workflow as well as 2) the dependency of your analysis results on version of intermediate steps in the pipeline (data as well as code).\n",
    "* **Collaboration & Sharing** Some day a collegue of yours wants to use your workflow or you are required to publish the analysis together with the manuscript presenting the results of your work to the scientific community. Instead of writing a book about which versions of what programm you installed in which order, wouldn't it be nice to have a structured and self explanatory of your analysis steps already at hand?\n",
    "* **Software Evolution** Luckily, software develops and bugs are fixed from time to time. Unfortunately this also implies that your workflow might break unexpectedly upon updating your system. Having a well defined environment to run your analysis in is essential for reproducibility of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snakemake  <img src=logos/snakemake_logo.svg alt=\"Drawing\" style=\"width: 100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snakemake helps you to structure your workflow by providing a framework to specify the dependencies between individual steps in your analysis workflow. By doing so it enforces a modular structure in the project and allows to specify the (python) software versions used in each step of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow definition according to snakemake is file based, i.e. each step in the workflow is specified via the required input files and the generated output files, as you might now from common [`Makefiles`](https://en.wikipedia.org/wiki/Make_(software)). Each step is defined in a `rule`, specifying input and output files as well as instructions on how to get from the input to the output files. Here the instructions to convert `fileA.txt` into `fileB.txt` are a simple copy performed in a shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule:\n",
    "    input: 'fileA.txt'\n",
    "    output: 'fileB.txt'\n",
    "    shell: 'cp fileA.txt fileB.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For snakemake the workflow definition needs to be specified in a `Snakefile` and can be executed by calling `snakemake` in a terminal in the same location as the `Snakefile`. Here the example rule above has been exported into a [`Snakefile`](Snakefile) using the `%%writefile` jupyter magic command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask snakemake to generate `fileB.txt` for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Nothing to be done.\n",
      "Complete log: /home/julia/presentations/2019-06-20_Toronto/snakemake_elephant_demo/.snakemake/log/2019-06-06T095229.961155.snakemake.log\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fails with snakemake complaining about\n",
    "```\n",
    "Missing input files for rule 1:\n",
    "fileA.txt\n",
    "```\n",
    "Which is correct, since there is no `fileA.txt` present to generate `fileB.txt` from. So let's add second rule which is capable of generating `fileA.txt` without required inputfiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Snakefile\n",
    "rule:\n",
    "    output: 'fileA.txt'\n",
    "    shell: 'touch fileA.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ask snakemake again to generate the `fileB.txt` for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Nothing to be done.\n",
      "Complete log: /home/julia/presentations/2019-06-20_Toronto/snakemake_elephant_demo/.snakemake/log/2019-06-06T095234.818879.snakemake.log\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally snakemake is first resolving the set of rules into a directed acyclic graph (dag) to determine in which order the rules neet do be executed. We can generate a visualization of the workflow using the `--dag` flag in combination with `dot` and `display` (for local notebook instances) or save the graph as svg (e.g. for remote instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag0.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting graph shows the dependencies between the two rules, which were automatically enumerated. The line style (continuous/dashed) indicated whether the rules were already executed or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](dag0.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide explicit names for rules to make the graph better human readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule copy_A_to_B:\n",
    "    input: 'fileA.txt'\n",
    "    output: 'fileB.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule create_A:\n",
    "    output: 'fileA.txt'\n",
    "    shell: 'touch fileA.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag1.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](dag1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we already used a different notation to specify in the shell comand `cp {input} {output}` instead of explicitely repeating the input and output filenames. These placeholders will be substituted by snakemake during the execution by the filenames defined as `input` / `output`. We can use the same notation to generalize the required input of the rule depending on the output, e.g. we permit the copy rule to work for arbitrary files having a certain naming scheme. Here a new folder `new_folder` is automatically generated for the copied files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule copy_to_new_folder:\n",
    "    input: 'original_data/file{id}.txt'\n",
    "    output: 'new_data/file{id}.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule create_file:\n",
    "    output: 'original_data/file{id}.txt'\n",
    "    shell: 'touch {output}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running the workflow now, we need to specify, which file we actually need as a final result and snakemake takes care of the individual steps to generate that file. We specify the desired output file as a snakemake argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n",
      "Using shell: /bin/bash\n",
      "Provided cores: 1\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tcopy_to_new_folder\n",
      "\t1\tcreate_file\n",
      "\t2\n",
      "\n",
      "[Fri Jun  7 12:11:44 2019]\n",
      "rule create_file:\n",
      "    output: original_data/fileZ.txt\n",
      "    jobid: 1\n",
      "    wildcards: id=Z\n",
      "\n",
      "[Fri Jun  7 12:11:44 2019]\n",
      "Finished job 1.\n",
      "1 of 2 steps (50%) done\n",
      "\n",
      "[Fri Jun  7 12:11:44 2019]\n",
      "rule copy_to_new_folder:\n",
      "    input: original_data/fileZ.txt\n",
      "    output: new_data/fileZ.txt\n",
      "    jobid: 0\n",
      "    wildcards: id=Z\n",
      "\n",
      "[Fri Jun  7 12:11:44 2019]\n",
      "Finished job 0.\n",
      "2 of 2 steps (100%) done\n",
      "Complete log: /home/julia/presentations/2019-06-20_Toronto/snakemake_elephant_demo/.snakemake/log/2019-06-07T121144.187150.snakemake.log\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake new_data/fileZ.txt --dag | dot | display\n",
    "snakemake new_data/fileZ.txt --dag | dot -Tsvg > dag2.svg\n",
    "snakemake new_data/fileZ.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a set of output files, we can either request these individually when running snakemake, e.g. using `snakemake -np new_folder/file{0,1,2,3,4,5,6,7,8,9}.txt`. In case the workflow output is not being changed frequently, it is also possible to add a final rule (conventionally named 'all'), which requests all desired output files of the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule all:\n",
    "    input: expand('new_data/file{id}.txt', id=range(10))\n",
    "rule copy_to_new_folder:\n",
    "    input: 'original_data/file{id}.txt'\n",
    "    output: 'new_data/file{id}.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule create_file:\n",
    "    output: 'original_data/file{id}.txt'\n",
    "    shell: 'touch {output}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag3.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used the snakemake function `expand`, which extends a given statement (here `new_folder_file{id}.txt`) for all combinations of parameters provided (here `id` values from 0 to 10). This permits to easily applied a set of rules to a number of different files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](dag3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, analysis is a bit more complicated than creating empty files and copying them from A to B using shell commands. Snakemake also support a number of different execution methods\n",
    "* in a shell (as used above)\n",
    "* in python (using run:)\n",
    "* run python/R/Markdown scripts directly (using script:)\n",
    "As an example we can use a small python script to generate our initial data files and store a (randomly generated) value. The Python script would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing generate_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_data.py\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def generate_random_data(output_filename):\n",
    "    # write a random number in an output file\n",
    "    f = open(output_filename, \"w\")\n",
    "    f.write(np.random.random())\n",
    "    f.close()\n",
    "\n",
    "# extracting the output filename from the command line parameters provided\n",
    "output_filename = sys.argv[1]\n",
    "generate_random_data(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding snakemake rule now needs to provide the argument to the `generate_data.py` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule all:\n",
    "    input: expand('new_data/file{id}.txt', id=range(10))\n",
    "rule copy_to_new_folder:\n",
    "    input: 'original_data/file{id}.txt'\n",
    "    output: 'new_data/file{id}.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule generate_data:\n",
    "    output: 'original_data/file{id}.txt'\n",
    "    run: 'generate_data.py {output}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag4.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional features worth having a look at\n",
    "* conda integration (--use-conda flag)\n",
    "* test runs (--dryrun)\n",
    "* ...\n",
    "* [snakemake documentation](https://snakemake.readthedocs.io) and [FAQ](https://snakemake.readthedocs.io/en/stable/project_info/faq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Snakemake for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present a simple data analysis workflow based on a published dataset of complex electrophysiology data using the electrophysiology analysis toolkit [Elephant](http://neuralensemble.org/elephant). Elephant is based on the [Neo](https://neo.readthedocs.io) data framework, which will first introduce based on artificially generatey spiking data with a known correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo - the data framework <img src=http://neo.readthedocs.org/en/latest/_images/neologo.png alt=\"NeoLogo\" style=\"width: 400px;\"/>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![NeoDataModel](https://neo.readthedocs.io/en/0.7.0/_images/base_schematic.png)\n",
    "*Neo* provides a set of classes for representing time series data (`SpikeTrain`, `AnalogSignalArray`, etc.), for representing the hierarchical arrangement of data in an experiment (`Segment`, `Block`) and for representing the relationship between spike trains and recording channels (`Unit`). Neo is used by a number of data visualization tools ([OpenElectrophy](http://neuralensemble.org/OpenElectrophy), [SpykeViewer](https://spyke-viewer.readthedocs.org/)) and by the [PyNN](http://neuralensemble.org/PyNN) metasimulator. Neo offers reading capabilities for a large number of proprietary electrophysiology data formats and conversion capability to generic open source data formats/models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neo data model consists of container (Block, Segment, ChannelIndex, Unit) and data objects (AnalogSignals, Spiketrains, Epoch, Events). Container objects provide the relation between the data stored in the Neo structure. Here, we consider only Block and Segment objects as containers. Blocks are designed to contain everything related to a whole recording session and link to Segments, which hold data belonging to a common time frame. For data objects we will use SpikeTrains, designed to capture the time series describing the occurrence of spikes together with the corresponding waveforms. All Neo data objects are derivatives of numpy arrays enhanced with the handling of physical quantities, minimal metadata as well as a generic mechanism to add custom metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the analysis requires minimum 3 steps:\n",
    "* getting / generating the data\n",
    "* running the main analysis\n",
    "* generating result plots\n",
    "\n",
    "Before running the analysis on experimental data, let's generate some data with known ground truth. Here Elephant provides a number of methods to generate spiketrain activity with defined statistical properties. The simplest would be to have independent spike time (Poisson process). Since we later want to detect higher order correlations, we will use a Poisson process as background activity and add correlated spikes to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data handling and visualization\n",
    "from quantities import Hz, ms\n",
    "from elephant.spike_train_generation import homogeneous_poisson_process, compound_poisson_process\n",
    "import neo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neo and Elephant are handling physical units consistently during the analysis by using the python module [quantities](https://python-quantities.readthedocs.io). This also requires parameters to be supplied in the correct dimension, such that the physical units can be matched during analysis. In general Neo objects capture all minimal information relevant for the interpretation of the data. In case of the spiketrain, this encompasses the start and stop times of recording/spike extraction as well the sampling_rate and potential custom annotations in form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spiketrain [108.03770006 238.1537939  265.83181284 281.17328539 317.61437496\n",
      " 339.18688686 354.92497774 388.12782541 719.64992206 722.00664199\n",
      " 735.24979501 877.06273956 937.38236616 949.62740249] ms\n",
      "Spiketrain attributes and physical units\n",
      "['t_start: 0.0', 't_stop: 1000.0', 'sampling_rate: 1.0', 'annotations: {}']\n",
      "['t_start: ms', 't_stop: ms', 'sampling_rate: Hz']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n"
     ]
    }
   ],
   "source": [
    "spiketrain = homogeneous_poisson_process(20*Hz, 0*ms, 1000*ms)\n",
    "print('The spiketrain', spiketrain)\n",
    "print('Spiketrain attributes and physical units')\n",
    "print(['{}: {}'.format(att, getattr(spiketrain,att)) for att in ['t_start', 't_stop', 'sampling_rate', 'annotations']])\n",
    "print(['{}: {}'.format(att, getattr(spiketrain,att).units.dimensionality) for att in ['t_start', 't_stop', 'sampling_rate']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An a first rule in or new workflow, let's generate multiple datasets with spiking activity and save them for future analysis steps. From the variety of file formats supported by Neo NIX has an hdf5 backend. Let's implement a virtual expiment, generating 100 poisson spiketrains stored in the NIX framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_data(output_filename):\n",
    "    io = neo.io.AsciiSpikeTrainIO(output_filename)\n",
    "    segment = neo.Segment(name='trial 0')\n",
    "    segment.spiketrains = [homogeneous_poisson_process(20*Hz, 0*ms, 1000*ms) for i in range(100)]\n",
    "    io.write_segment(segment)\n",
    "    \n",
    "generate_poisson_data('original_data/dataset0.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export this piece of code into a standalone script so we can use it in the snakemake workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate_poisson.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_poisson.py\n",
    "import sys\n",
    "import neo\n",
    "def generate_poisson_data(output_filename):\n",
    "    io = neo.io.AsciiSpikeTrainIO(output_filename)\n",
    "    segment = neo.Segment(name='trial 0')\n",
    "    segment.spiketrains = [homogeneous_poisson_process(20*Hz, 0*ms, 1000*ms) for i in range(100)]\n",
    "    io.write_segment(segment)\n",
    "    \n",
    "generate_poisson_data(sys.argv[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first rule in the workflow know how to utilize the script to generate new datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule all:\n",
    "    input: expand('original_data/dataset{id}.txt', id=range(10))\n",
    "        \n",
    "rule generate_data:\n",
    "    output: 'original_data/dataset{id}.txt'\n",
    "    run: 'generate_poisson_data.py {output}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag5.svg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i, spiketrain in enumerate(spiketrains):\n",
    "            plt.plot(spiketrain, [i] * len(spiketrain), '.')\n",
    "plt.xlabel('Time ({})'.format(spiketrains[0].times.dimensionality.latex))\n",
    "plt.ylabel('Spiketrains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all spiketimes are uncorrelated, which would make this dataset a bit boring for analysis. To introduce some correlations, while keeping all other aspects constant Elephant provides the option to generate a compound poisson process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = compound_poisson_process(rate=5*pq.Hz, A=[0]+[0.99]+[0]*9+[0.01], t_stop=10*pq.s)\n",
    "for i in range(89):\n",
    "    sts.append(homogeneous_poisson_process(rate=5*pq.Hz, t_stop=10*pq.s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Neo: electrophysiology data objects, I/O**\n",
    "<img src=http://neo.readthedocs.org/en/latest/_images/neologo.png alt=\"Drawing\" style=\"width: 400px;\"/>    \n",
    "\n",
    "**Elephant: electrophysiology data analysis**\n",
    "<img src=http://elephant.readthedocs.org/en/latest/_static/elephant_logo_sidebar.png alt=\"Drawing\" style=\"width: 300px;\"/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Elephant* is a data analysis library built on Neo. It aims to provide a standard library for electrophysiology data analysis, merging functions from OpenElectrophy, SpykeViewer and [NeuroTools](http://neuralensemble.org/NeuroTools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOADD\n",
    "SNAKEMAKE\n",
    "* wildcards (done)\n",
    "* different rule execution statements (done)\n",
    "* conda environments\n",
    "* expand (done)\n",
    "* config\n",
    "* rulegraph\n",
    "NEO\n",
    "* class structure\n",
    "* io overview\n",
    "* intro to main data objects\n",
    "ELEPHANT\n",
    "* spade elephant demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
