{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snakemake Introduction to Elephant\n",
    "## Workflow management based on a electrophysiology example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table  bgcolor=\"#000000\"><tr>\n",
    "<td><img src=logos/snakemake_logo.svg alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "<td><img src=http://neo.readthedocs.org/en/latest/_images/neologo.png alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "<td><img src=http://elephant.readthedocs.org/en/latest/_static/elephant_logo_sidebar.png alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial I will first introduce the workflow management system [snakemake](https://snakemake.readthedocs.io) and show how this can be used for analysis of electrophysiology data using the electrophysiology analysis toolkit [Elephant](http://neuralensemble.org/elephant). For snakemake as well as Elephant tutorials exist already as part of the documentation and were partially reused in the tutorial presented here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE: When running this tutorial locally, please first install the [requirements](environment.yml) to ensure the examples are working.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow management - Do I need this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a number of reasons for managing your managing workflows besides the classical 'I always run first script a and then script b':\n",
    "* **Growing Complexity** When starting a seemingly small and simple project everything is still pretty easy, but typically projects tend to grow beyond the initially expected size and complexity. This affects two different aspects 1) the dependency between different steps in workflow as well as 2) the dependency of your analysis results on version of intermediate steps in the pipeline (data as well as code).\n",
    "* **Collaboration & Sharing** Some day a collegue of yours wants to use your workflow or you are required to publish the analysis together with the manuscript presenting the results of your work to the scientific community. Instead of writing a book about which versions of what programm you installed in which order, wouldn't it be nice to have a structured and self explanatory of your analysis steps already at hand?\n",
    "* **Software Evolution** Luckily, software develops and bugs are fixed from time to time. Unfortunately this also implies that your workflow might break unexpectedly upon updating your system. Having a well defined environment to run your analysis in is essential for reproducibility of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snakemake  <img src=logos/snakemake_logo.svg alt=\"Drawing\" style=\"width: 100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snakemake helps you to structure your workflow by providing a framework to specify the dependencies between individual steps in your analysis workflow. By doing so it enforces a modular structure in the project and allows to specify the (python) software versions used in each step of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow definition according to snakemake is file based, i.e. each step in the workflow is specified via the required input files and the generated output files, as you might now from common [`Makefiles`](https://en.wikipedia.org/wiki/Make_(software)). Each step is defined in a `rule`, specifying input and output files as well as instructions on how to get from the input to the output files. Here the instructions to convert `fileA.txt` into `fileB.txt` are a simple copy performed in a shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule:\n",
    "    input: 'fileA.txt'\n",
    "    output: 'fileB.txt'\n",
    "    shell: 'cp fileA.txt fileB.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For snakemake the workflow definition needs to be specified in a `Snakefile` and can be executed by calling `snakemake` in a terminal in the same location as the `Snakefile`. Here the example rule above has been exported into a [`Snakefile`](Snakefile) using the `%%writefile` jupyter magic command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can ask snakemake to generate `fileB.txt` for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "MissingInputException in line 1 of /home/julia/presentations/2019-06-20_Toronto/snakemake_elephant_demo/Snakefile:\n",
      "Missing input files for rule 1:\n",
      "fileA.txt\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'snakemake\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6e988fab01d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sh'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'snakemake\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/demo/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2350\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2351\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2352\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/demo/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m</home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/decorator.py:decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/demo/lib/python3.6/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/demo/lib/python3.6/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'snakemake\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fails with snakemake complaining about\n",
    "```\n",
    "Missing input files for rule 1:\n",
    "fileA.txt\n",
    "```\n",
    "Which is correct, since there is no `fileA.txt` present to generate `fileB.txt` from. So let's add second rule which is capable of generating `fileA.txt` without required inputfiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Snakefile\n",
    "rule:\n",
    "    output: 'fileA.txt'\n",
    "    shell: 'touch fileA.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And ask snakemake again to generate the `fileB.txt` for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Using shell: /bin/bash\n",
      "Provided cores: 1\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\t1\n",
      "\t1\t2\n",
      "\t2\n",
      "\n",
      "[Sun Jun  9 09:32:18 2019]\n",
      "rule 2:\n",
      "    output: fileA.txt\n",
      "    jobid: 1\n",
      "\n",
      "[Sun Jun  9 09:32:18 2019]\n",
      "Finished job 1.\n",
      "1 of 2 steps (50%) done\n",
      "\n",
      "[Sun Jun  9 09:32:18 2019]\n",
      "rule 1:\n",
      "    input: fileA.txt\n",
      "    output: fileB.txt\n",
      "    jobid: 0\n",
      "\n",
      "[Sun Jun  9 09:32:18 2019]\n",
      "Finished job 0.\n",
      "2 of 2 steps (100%) done\n",
      "Complete log: /home/julia/presentations/2019-06-20_Toronto/snakemake_elephant_demo/.snakemake/log/2019-06-09T093218.538609.snakemake.log\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally snakemake is first resolving the set of rules into a directed acyclic graph (dag) to determine in which order the rules neet do be executed. We can generate a visualization of the workflow using the `--dag` flag in combination with `dot` and `display` (for local notebook instances) or save the graph as svg (e.g. for remote instances)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag0.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting graph shows the dependencies between the two rules, which were automatically enumerated. The line style (continuous/dashed) indicated whether the rules were already executed or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](dag0.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide explicit names for rules to make the graph better human readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule copy_A_to_B:\n",
    "    input: 'fileA.txt'\n",
    "    output: 'fileB.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule create_A:\n",
    "    output: 'fileA.txt'\n",
    "    shell: 'touch fileA.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag1.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](dag1.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we already used a different notation to specify in the shell comand `cp {input} {output}` instead of explicitely repeating the input and output filenames. These placeholders will be substituted by snakemake during the execution by the filenames defined as `input` / `output`. We can use the same notation to generalize the required input of the rule depending on the output, e.g. we permit the copy rule to work for arbitrary files having a certain naming scheme. Here a new folder `new_folder` is automatically generated for the copied files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule copy_to_new_folder:\n",
    "    input: 'original_data/file{id}.txt'\n",
    "    output: 'new_data/file{id}.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule create_file:\n",
    "    output: 'original_data/file{id}.txt'\n",
    "    shell: 'touch {output}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For running the workflow now, we need to specify, which file we actually need as a final result and snakemake takes care of the individual steps to generate that file. We specify the desired output file as a snakemake argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n",
      "Nothing to be done.\n",
      "Complete log: /home/julia/presentations/2019-06-20_Toronto/snakemake_elephant_demo/.snakemake/log/2019-06-09T093246.182039.snakemake.log\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake new_data/fileZ.txt --dag | dot | display\n",
    "snakemake new_data/fileZ.txt --dag | dot -Tsvg > dag2.svg\n",
    "snakemake new_data/fileZ.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a set of output files, we can either request these individually when running snakemake, e.g. using `snakemake -np new_folder/file{0,1,2,3,4,5,6,7,8,9}.txt`. In case the workflow output is not being changed frequently, it is also possible to add a final rule (conventionally named 'all'), which requests all desired output files of the workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule all:\n",
    "    input: expand('new_data/file{id}.txt', id=range(10))\n",
    "rule copy_to_new_folder:\n",
    "    input: 'original_data/file{id}.txt'\n",
    "    output: 'new_data/file{id}.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule create_file:\n",
    "    output: 'original_data/file{id}.txt'\n",
    "    shell: 'touch {output}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag3.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I used the snakemake function `expand`, which extends a given statement (here `new_folder_file{id}.txt`) for all combinations of parameters provided (here `id` values from 0 to 10). This permits to easily applied a set of rules to a number of different files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](dag3.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, analysis is a bit more complicated than creating empty files and copying them from A to B using shell commands. Snakemake also support a number of different execution methods\n",
    "* in a shell (as used above)\n",
    "* in python (using run:)\n",
    "* run python/R/Markdown scripts directly (using script:)\n",
    "As an example we can use a small python script to generate our initial data files and store a (randomly generated) value. The Python script would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_data.py\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "def generate_random_data(output_filename):\n",
    "    # write a random number in an output file\n",
    "    f = open(output_filename, \"w\")\n",
    "    f.write(np.random.random())\n",
    "    f.close()\n",
    "\n",
    "# extracting the output filename from the command line parameters provided\n",
    "output_filename = sys.argv[1]\n",
    "generate_random_data(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding snakemake rule now needs to provide the argument to the `generate_data.py` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "rule all:\n",
    "    input: expand('new_data/file{id}.txt', id=range(10))\n",
    "rule copy_to_new_folder:\n",
    "    input: 'original_data/file{id}.txt'\n",
    "    output: 'new_data/file{id}.txt'\n",
    "    shell: 'cp {input} {output}'\n",
    "rule generate_data:\n",
    "    output: 'original_data/file{id}.txt'\n",
    "    run: 'generate_data.py {output}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag4.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional features worth having a look at\n",
    "* conda integration (--use-conda flag)\n",
    "* test runs (--dryrun)\n",
    "* print shell commands (-p)\n",
    "* ...\n",
    "* [snakemake documentation](https://snakemake.readthedocs.io) and [FAQ](https://snakemake.readthedocs.io/en/stable/project_info/faq.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing Snakemake for Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will present a simple data analysis workflow based on a published dataset of complex electrophysiology data using the electrophysiology analysis toolkit [Elephant](http://neuralensemble.org/elephant). Elephant is based on the [Neo](https://neo.readthedocs.io) data framework, which will first introduce based on artificially generatey spiking data with a known correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo - the data framework <img src=http://neo.readthedocs.org/en/latest/_images/neologo.png alt=\"NeoLogo\" style=\"width: 400px;\"/>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![NeoDataModel](https://neo.readthedocs.io/en/0.7.0/_images/base_schematic.png)\n",
    "*Neo* provides a set of classes for representing time series data (`SpikeTrain`, `AnalogSignalArray`, etc.), for representing the hierarchical arrangement of data in an experiment (`Segment`, `Block`) and for representing the relationship between spike trains and recording channels (`Unit`). Neo is used by a number of data visualization tools ([OpenElectrophy](http://neuralensemble.org/OpenElectrophy), [SpykeViewer](https://spyke-viewer.readthedocs.org/)) and by the [PyNN](http://neuralensemble.org/PyNN) metasimulator. Neo offers reading capabilities for a large number of proprietary electrophysiology data formats and conversion capability to generic open source data formats/models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neo data model consists of container (Block, Segment, ChannelIndex, Unit) and data objects (AnalogSignals, Spiketrains, Epoch, Events). Container objects provide the relation between the data stored in the Neo structure. Here, we consider only Block and Segment objects as containers. Blocks are designed to contain everything related to a whole recording session and link to Segments, which hold data belonging to a common time frame. For data objects we will use SpikeTrains, designed to capture the time series describing the occurrence of spikes together with the corresponding waveforms. All Neo data objects are derivatives of numpy arrays enhanced with the handling of physical quantities, minimal metadata as well as a generic mechanism to add custom metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the analysis requires minimum 3 steps:\n",
    "* getting / generating the data\n",
    "* running the main analysis\n",
    "* generating result plots\n",
    "\n",
    "Before running the analysis on experimental data, let's generate some data with known ground truth. Here Elephant provides a number of methods to generate spiketrain activity with defined statistical properties. The simplest would be to have independent spike time (Poisson process). Since we later want to detect higher order correlations, we will use a Poisson process as background activity and add correlated spikes to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for data handling and visualization\n",
    "from quantities import Hz, ms\n",
    "from elephant.spike_train_generation import homogeneous_poisson_process, compound_poisson_process\n",
    "import neo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neo and Elephant are handling physical units consistently during the analysis by using the python module [quantities](https://python-quantities.readthedocs.io). This also requires parameters to be supplied in the correct dimension, such that the physical units can be matched during analysis. In general Neo objects capture all minimal information relevant for the interpretation of the data. In case of the spiketrain, this encompasses the start and stop times of recording/spike extraction as well the sampling_rate and potential custom annotations in form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spiketrain [ 36.93074588  61.6662392   88.98177318 130.79297446 162.72648138\n",
      " 214.97093946 281.5373308  299.02891039 443.91821042 446.421389\n",
      " 470.33935321 517.56473044 718.32428799 762.97998738 796.33587736\n",
      " 881.82476674 924.69677941] ms\n",
      "Spiketrain attributes and physical units\n",
      "['t_start: 0.0', 't_stop: 1000.0', 'sampling_rate: 1.0', 'annotations: {}']\n",
      "['t_start: ms', 't_stop: ms', 'sampling_rate: Hz']\n"
     ]
    }
   ],
   "source": [
    "spiketrain = homogeneous_poisson_process(20*Hz, 0*ms, 1000*ms)\n",
    "print('The spiketrain', spiketrain)\n",
    "print('Spiketrain attributes and physical units')\n",
    "print(['{}: {}'.format(att, getattr(spiketrain,att)) for att in ['t_start', 't_stop', 'sampling_rate', 'annotations']])\n",
    "print(['{}: {}'.format(att, getattr(spiketrain,att).units.dimensionality) for att in ['t_start', 't_stop', 'sampling_rate']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An a first rule in or new workflow, let's generate multiple datasets with spiking activity and save them for future analysis steps. From the variety of file formats supported by Neo NIX has an hdf5 backend. Let's implement a virtual expiment, generating 100 poisson spiketrains stored in the NIX framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_poisson_data(output_filename):\n",
    "    io = neo.io.AsciiSpikeTrainIO(output_filename)\n",
    "    segment = neo.Segment(name='trial 0')\n",
    "    segment.spiketrains.append(homogeneous_poisson_process(20*Hz, 0*ms, 1000*ms))\n",
    "    io.write_segment(segment)\n",
    "    \n",
    "generate_poisson_data('original_data/example_spiketrain.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export this piece of code into a standalone script so we can use it in the snakemake workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate_poisson.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_poisson.py\n",
    "import sys\n",
    "import neo\n",
    "from os.path import join\n",
    "from quantities import Hz, ms\n",
    "from elephant.spike_train_generation import homogeneous_poisson_process\n",
    "def generate_poisson_data(output_folder, n=10):\n",
    "    for i in range(n):\n",
    "        io = neo.io.AsciiSpikeTrainIO('{}/spiketrain-{}.txt'.format(output_folder, i))\n",
    "        segment = neo.Segment()\n",
    "        segment.spiketrains = [homogeneous_poisson_process(20*Hz, 0*ms, 1000*ms).rescale('s')]\n",
    "        io.write_segment(segment)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    generate_poisson_data(*sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_poisson import generate_poisson_data\n",
    "from os import mkdir\n",
    "mkdir('original_data/dataset6')\n",
    "generate_poisson_data('original_data/dataset6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first rule in the workflow know how to utilize the script to generate new datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Snakefile\n",
    "wildcard_constraints:\n",
    "    id=\"\\d+\"\n",
    "\n",
    "rule all:\n",
    "    input: expand('original_data/dataset{id}/spiketrain-{i}.txt', id=range(10), i=range(10))\n",
    "        \n",
    "rule generate_data:\n",
    "    output: expand('original_data/dataset{{id}}/spiketrain-{i}.txt', i=range(10))\n",
    "    params: dir_name='original_data/dataset{id}'\n",
    "    shell: 'python generate_poisson.py {params.dir_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And running the workflow. The output files should appear [here](original_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Using shell: /bin/bash\n",
      "Provided cores: 1\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tall\n",
      "\t9\tgenerate_data\n",
      "\t10\n",
      "\n",
      "[Sun Jun  9 09:53:34 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset3/spiketrain-0.txt, original_data/dataset3/spiketrain-1.txt, original_data/dataset3/spiketrain-2.txt, original_data/dataset3/spiketrain-3.txt, original_data/dataset3/spiketrain-4.txt, original_data/dataset3/spiketrain-5.txt, original_data/dataset3/spiketrain-6.txt, original_data/dataset3/spiketrain-7.txt, original_data/dataset3/spiketrain-8.txt, original_data/dataset3/spiketrain-9.txt\n",
      "    jobid: 4\n",
      "    reason: Missing output files: original_data/dataset3/spiketrain-0.txt, original_data/dataset3/spiketrain-5.txt, original_data/dataset3/spiketrain-7.txt, original_data/dataset3/spiketrain-8.txt, original_data/dataset3/spiketrain-9.txt, original_data/dataset3/spiketrain-3.txt, original_data/dataset3/spiketrain-1.txt, original_data/dataset3/spiketrain-4.txt, original_data/dataset3/spiketrain-6.txt, original_data/dataset3/spiketrain-2.txt\n",
      "    wildcards: id=3\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset3\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:35 2019]\n",
      "Finished job 4.\n",
      "1 of 10 steps (10%) done\n",
      "\n",
      "[Sun Jun  9 09:53:35 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset7/spiketrain-0.txt, original_data/dataset7/spiketrain-1.txt, original_data/dataset7/spiketrain-2.txt, original_data/dataset7/spiketrain-3.txt, original_data/dataset7/spiketrain-4.txt, original_data/dataset7/spiketrain-5.txt, original_data/dataset7/spiketrain-6.txt, original_data/dataset7/spiketrain-7.txt, original_data/dataset7/spiketrain-8.txt, original_data/dataset7/spiketrain-9.txt\n",
      "    jobid: 8\n",
      "    reason: Missing output files: original_data/dataset7/spiketrain-1.txt, original_data/dataset7/spiketrain-7.txt, original_data/dataset7/spiketrain-8.txt, original_data/dataset7/spiketrain-4.txt, original_data/dataset7/spiketrain-2.txt, original_data/dataset7/spiketrain-6.txt, original_data/dataset7/spiketrain-3.txt, original_data/dataset7/spiketrain-9.txt, original_data/dataset7/spiketrain-0.txt, original_data/dataset7/spiketrain-5.txt\n",
      "    wildcards: id=7\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset7\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:36 2019]\n",
      "Finished job 8.\n",
      "2 of 10 steps (20%) done\n",
      "\n",
      "[Sun Jun  9 09:53:36 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset1/spiketrain-0.txt, original_data/dataset1/spiketrain-1.txt, original_data/dataset1/spiketrain-2.txt, original_data/dataset1/spiketrain-3.txt, original_data/dataset1/spiketrain-4.txt, original_data/dataset1/spiketrain-5.txt, original_data/dataset1/spiketrain-6.txt, original_data/dataset1/spiketrain-7.txt, original_data/dataset1/spiketrain-8.txt, original_data/dataset1/spiketrain-9.txt\n",
      "    jobid: 2\n",
      "    reason: Missing output files: original_data/dataset1/spiketrain-0.txt, original_data/dataset1/spiketrain-1.txt, original_data/dataset1/spiketrain-7.txt, original_data/dataset1/spiketrain-8.txt, original_data/dataset1/spiketrain-2.txt, original_data/dataset1/spiketrain-3.txt, original_data/dataset1/spiketrain-4.txt, original_data/dataset1/spiketrain-6.txt, original_data/dataset1/spiketrain-9.txt, original_data/dataset1/spiketrain-5.txt\n",
      "    wildcards: id=1\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset1\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:38 2019]\n",
      "Finished job 2.\n",
      "3 of 10 steps (30%) done\n",
      "\n",
      "[Sun Jun  9 09:53:38 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset0/spiketrain-0.txt, original_data/dataset0/spiketrain-1.txt, original_data/dataset0/spiketrain-2.txt, original_data/dataset0/spiketrain-3.txt, original_data/dataset0/spiketrain-4.txt, original_data/dataset0/spiketrain-5.txt, original_data/dataset0/spiketrain-6.txt, original_data/dataset0/spiketrain-7.txt, original_data/dataset0/spiketrain-8.txt, original_data/dataset0/spiketrain-9.txt\n",
      "    jobid: 1\n",
      "    reason: Missing output files: original_data/dataset0/spiketrain-8.txt, original_data/dataset0/spiketrain-0.txt, original_data/dataset0/spiketrain-9.txt, original_data/dataset0/spiketrain-3.txt, original_data/dataset0/spiketrain-1.txt, original_data/dataset0/spiketrain-7.txt, original_data/dataset0/spiketrain-5.txt, original_data/dataset0/spiketrain-2.txt, original_data/dataset0/spiketrain-6.txt, original_data/dataset0/spiketrain-4.txt\n",
      "    wildcards: id=0\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset0\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:39 2019]\n",
      "Finished job 1.\n",
      "4 of 10 steps (40%) done\n",
      "\n",
      "[Sun Jun  9 09:53:39 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset9/spiketrain-0.txt, original_data/dataset9/spiketrain-1.txt, original_data/dataset9/spiketrain-2.txt, original_data/dataset9/spiketrain-3.txt, original_data/dataset9/spiketrain-4.txt, original_data/dataset9/spiketrain-5.txt, original_data/dataset9/spiketrain-6.txt, original_data/dataset9/spiketrain-7.txt, original_data/dataset9/spiketrain-8.txt, original_data/dataset9/spiketrain-9.txt\n",
      "    jobid: 10\n",
      "    reason: Missing output files: original_data/dataset9/spiketrain-3.txt, original_data/dataset9/spiketrain-2.txt, original_data/dataset9/spiketrain-1.txt, original_data/dataset9/spiketrain-0.txt, original_data/dataset9/spiketrain-9.txt, original_data/dataset9/spiketrain-4.txt, original_data/dataset9/spiketrain-5.txt, original_data/dataset9/spiketrain-6.txt, original_data/dataset9/spiketrain-8.txt, original_data/dataset9/spiketrain-7.txt\n",
      "    wildcards: id=9\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset9\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:40 2019]\n",
      "Finished job 10.\n",
      "5 of 10 steps (50%) done\n",
      "\n",
      "[Sun Jun  9 09:53:40 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset8/spiketrain-0.txt, original_data/dataset8/spiketrain-1.txt, original_data/dataset8/spiketrain-2.txt, original_data/dataset8/spiketrain-3.txt, original_data/dataset8/spiketrain-4.txt, original_data/dataset8/spiketrain-5.txt, original_data/dataset8/spiketrain-6.txt, original_data/dataset8/spiketrain-7.txt, original_data/dataset8/spiketrain-8.txt, original_data/dataset8/spiketrain-9.txt\n",
      "    jobid: 9\n",
      "    reason: Missing output files: original_data/dataset8/spiketrain-8.txt, original_data/dataset8/spiketrain-5.txt, original_data/dataset8/spiketrain-2.txt, original_data/dataset8/spiketrain-7.txt, original_data/dataset8/spiketrain-4.txt, original_data/dataset8/spiketrain-6.txt, original_data/dataset8/spiketrain-9.txt, original_data/dataset8/spiketrain-0.txt, original_data/dataset8/spiketrain-3.txt, original_data/dataset8/spiketrain-1.txt\n",
      "    wildcards: id=8\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset8\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:41 2019]\n",
      "Finished job 9.\n",
      "6 of 10 steps (60%) done\n",
      "\n",
      "[Sun Jun  9 09:53:41 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset5/spiketrain-0.txt, original_data/dataset5/spiketrain-1.txt, original_data/dataset5/spiketrain-2.txt, original_data/dataset5/spiketrain-3.txt, original_data/dataset5/spiketrain-4.txt, original_data/dataset5/spiketrain-5.txt, original_data/dataset5/spiketrain-6.txt, original_data/dataset5/spiketrain-7.txt, original_data/dataset5/spiketrain-8.txt, original_data/dataset5/spiketrain-9.txt\n",
      "    jobid: 6\n",
      "    reason: Missing output files: original_data/dataset5/spiketrain-4.txt, original_data/dataset5/spiketrain-6.txt, original_data/dataset5/spiketrain-9.txt, original_data/dataset5/spiketrain-3.txt, original_data/dataset5/spiketrain-1.txt, original_data/dataset5/spiketrain-8.txt, original_data/dataset5/spiketrain-7.txt, original_data/dataset5/spiketrain-5.txt, original_data/dataset5/spiketrain-0.txt, original_data/dataset5/spiketrain-2.txt\n",
      "    wildcards: id=5\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset5\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:42 2019]\n",
      "Finished job 6.\n",
      "7 of 10 steps (70%) done\n",
      "\n",
      "[Sun Jun  9 09:53:42 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset2/spiketrain-0.txt, original_data/dataset2/spiketrain-1.txt, original_data/dataset2/spiketrain-2.txt, original_data/dataset2/spiketrain-3.txt, original_data/dataset2/spiketrain-4.txt, original_data/dataset2/spiketrain-5.txt, original_data/dataset2/spiketrain-6.txt, original_data/dataset2/spiketrain-7.txt, original_data/dataset2/spiketrain-8.txt, original_data/dataset2/spiketrain-9.txt\n",
      "    jobid: 3\n",
      "    reason: Missing output files: original_data/dataset2/spiketrain-1.txt, original_data/dataset2/spiketrain-6.txt, original_data/dataset2/spiketrain-8.txt, original_data/dataset2/spiketrain-4.txt, original_data/dataset2/spiketrain-9.txt, original_data/dataset2/spiketrain-0.txt, original_data/dataset2/spiketrain-3.txt, original_data/dataset2/spiketrain-2.txt, original_data/dataset2/spiketrain-5.txt, original_data/dataset2/spiketrain-7.txt\n",
      "    wildcards: id=2\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset2\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:43 2019]\n",
      "Finished job 3.\n",
      "8 of 10 steps (80%) done\n",
      "\n",
      "[Sun Jun  9 09:53:43 2019]\n",
      "rule generate_data:\n",
      "    output: original_data/dataset4/spiketrain-0.txt, original_data/dataset4/spiketrain-1.txt, original_data/dataset4/spiketrain-2.txt, original_data/dataset4/spiketrain-3.txt, original_data/dataset4/spiketrain-4.txt, original_data/dataset4/spiketrain-5.txt, original_data/dataset4/spiketrain-6.txt, original_data/dataset4/spiketrain-7.txt, original_data/dataset4/spiketrain-8.txt, original_data/dataset4/spiketrain-9.txt\n",
      "    jobid: 5\n",
      "    reason: Missing output files: original_data/dataset4/spiketrain-2.txt, original_data/dataset4/spiketrain-4.txt, original_data/dataset4/spiketrain-0.txt, original_data/dataset4/spiketrain-7.txt, original_data/dataset4/spiketrain-8.txt, original_data/dataset4/spiketrain-5.txt, original_data/dataset4/spiketrain-6.txt, original_data/dataset4/spiketrain-1.txt, original_data/dataset4/spiketrain-9.txt, original_data/dataset4/spiketrain-3.txt\n",
      "    wildcards: id=4\n",
      "\n",
      "\n",
      "    python generate_poisson.py original_data/dataset4\n",
      "    \n",
      "/home/julia/anaconda3/envs/demo/lib/python3.6/site-packages/quantities/quantity.py:321: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  return np.true_divide(other, self)\n",
      "[Sun Jun  9 09:53:45 2019]\n",
      "Finished job 5.\n",
      "9 of 10 steps (90%) done\n",
      "\n",
      "[Sun Jun  9 09:53:45 2019]\n",
      "localrule all:\n",
      "    input: original_data/dataset0/spiketrain-0.txt, original_data/dataset0/spiketrain-1.txt, original_data/dataset0/spiketrain-2.txt, original_data/dataset0/spiketrain-3.txt, original_data/dataset0/spiketrain-4.txt, original_data/dataset0/spiketrain-5.txt, original_data/dataset0/spiketrain-6.txt, original_data/dataset0/spiketrain-7.txt, original_data/dataset0/spiketrain-8.txt, original_data/dataset0/spiketrain-9.txt, original_data/dataset1/spiketrain-0.txt, original_data/dataset1/spiketrain-1.txt, original_data/dataset1/spiketrain-2.txt, original_data/dataset1/spiketrain-3.txt, original_data/dataset1/spiketrain-4.txt, original_data/dataset1/spiketrain-5.txt, original_data/dataset1/spiketrain-6.txt, original_data/dataset1/spiketrain-7.txt, original_data/dataset1/spiketrain-8.txt, original_data/dataset1/spiketrain-9.txt, original_data/dataset2/spiketrain-0.txt, original_data/dataset2/spiketrain-1.txt, original_data/dataset2/spiketrain-2.txt, original_data/dataset2/spiketrain-3.txt, original_data/dataset2/spiketrain-4.txt, original_data/dataset2/spiketrain-5.txt, original_data/dataset2/spiketrain-6.txt, original_data/dataset2/spiketrain-7.txt, original_data/dataset2/spiketrain-8.txt, original_data/dataset2/spiketrain-9.txt, original_data/dataset3/spiketrain-0.txt, original_data/dataset3/spiketrain-1.txt, original_data/dataset3/spiketrain-2.txt, original_data/dataset3/spiketrain-3.txt, original_data/dataset3/spiketrain-4.txt, original_data/dataset3/spiketrain-5.txt, original_data/dataset3/spiketrain-6.txt, original_data/dataset3/spiketrain-7.txt, original_data/dataset3/spiketrain-8.txt, original_data/dataset3/spiketrain-9.txt, original_data/dataset4/spiketrain-0.txt, original_data/dataset4/spiketrain-1.txt, original_data/dataset4/spiketrain-2.txt, original_data/dataset4/spiketrain-3.txt, original_data/dataset4/spiketrain-4.txt, original_data/dataset4/spiketrain-5.txt, original_data/dataset4/spiketrain-6.txt, original_data/dataset4/spiketrain-7.txt, original_data/dataset4/spiketrain-8.txt, original_data/dataset4/spiketrain-9.txt, original_data/dataset5/spiketrain-0.txt, original_data/dataset5/spiketrain-1.txt, original_data/dataset5/spiketrain-2.txt, original_data/dataset5/spiketrain-3.txt, original_data/dataset5/spiketrain-4.txt, original_data/dataset5/spiketrain-5.txt, original_data/dataset5/spiketrain-6.txt, original_data/dataset5/spiketrain-7.txt, original_data/dataset5/spiketrain-8.txt, original_data/dataset5/spiketrain-9.txt, original_data/dataset6/spiketrain-0.txt, original_data/dataset6/spiketrain-1.txt, original_data/dataset6/spiketrain-2.txt, original_data/dataset6/spiketrain-3.txt, original_data/dataset6/spiketrain-4.txt, original_data/dataset6/spiketrain-5.txt, original_data/dataset6/spiketrain-6.txt, original_data/dataset6/spiketrain-7.txt, original_data/dataset6/spiketrain-8.txt, original_data/dataset6/spiketrain-9.txt, original_data/dataset7/spiketrain-0.txt, original_data/dataset7/spiketrain-1.txt, original_data/dataset7/spiketrain-2.txt, original_data/dataset7/spiketrain-3.txt, original_data/dataset7/spiketrain-4.txt, original_data/dataset7/spiketrain-5.txt, original_data/dataset7/spiketrain-6.txt, original_data/dataset7/spiketrain-7.txt, original_data/dataset7/spiketrain-8.txt, original_data/dataset7/spiketrain-9.txt, original_data/dataset8/spiketrain-0.txt, original_data/dataset8/spiketrain-1.txt, original_data/dataset8/spiketrain-2.txt, original_data/dataset8/spiketrain-3.txt, original_data/dataset8/spiketrain-4.txt, original_data/dataset8/spiketrain-5.txt, original_data/dataset8/spiketrain-6.txt, original_data/dataset8/spiketrain-7.txt, original_data/dataset8/spiketrain-8.txt, original_data/dataset8/spiketrain-9.txt, original_data/dataset9/spiketrain-0.txt, original_data/dataset9/spiketrain-1.txt, original_data/dataset9/spiketrain-2.txt, original_data/dataset9/spiketrain-3.txt, original_data/dataset9/spiketrain-4.txt, original_data/dataset9/spiketrain-5.txt, original_data/dataset9/spiketrain-6.txt, original_data/dataset9/spiketrain-7.txt, original_data/dataset9/spiketrain-8.txt, original_data/dataset9/spiketrain-9.txt\n",
      "    jobid: 0\n",
      "    reason: Input files updated by another job: original_data/dataset8/spiketrain-9.txt, original_data/dataset9/spiketrain-6.txt, original_data/dataset8/spiketrain-8.txt, original_data/dataset4/spiketrain-5.txt, original_data/dataset3/spiketrain-4.txt, original_data/dataset8/spiketrain-3.txt, original_data/dataset3/spiketrain-2.txt, original_data/dataset9/spiketrain-7.txt, original_data/dataset5/spiketrain-9.txt, original_data/dataset1/spiketrain-7.txt, original_data/dataset1/spiketrain-3.txt, original_data/dataset2/spiketrain-2.txt, original_data/dataset5/spiketrain-5.txt, original_data/dataset4/spiketrain-9.txt, original_data/dataset1/spiketrain-0.txt, original_data/dataset0/spiketrain-7.txt, original_data/dataset7/spiketrain-6.txt, original_data/dataset4/spiketrain-1.txt, original_data/dataset5/spiketrain-7.txt, original_data/dataset4/spiketrain-3.txt, original_data/dataset7/spiketrain-4.txt, original_data/dataset5/spiketrain-2.txt, original_data/dataset9/spiketrain-3.txt, original_data/dataset4/spiketrain-7.txt, original_data/dataset2/spiketrain-0.txt, original_data/dataset4/spiketrain-6.txt, original_data/dataset8/spiketrain-0.txt, original_data/dataset1/spiketrain-9.txt, original_data/dataset9/spiketrain-1.txt, original_data/dataset8/spiketrain-6.txt, original_data/dataset1/spiketrain-5.txt, original_data/dataset0/spiketrain-8.txt, original_data/dataset1/spiketrain-1.txt, original_data/dataset5/spiketrain-6.txt, original_data/dataset0/spiketrain-1.txt, original_data/dataset0/spiketrain-5.txt, original_data/dataset5/spiketrain-0.txt, original_data/dataset3/spiketrain-7.txt, original_data/dataset5/spiketrain-1.txt, original_data/dataset9/spiketrain-8.txt, original_data/dataset2/spiketrain-7.txt, original_data/dataset1/spiketrain-8.txt, original_data/dataset1/spiketrain-2.txt, original_data/dataset2/spiketrain-3.txt, original_data/dataset7/spiketrain-9.txt, original_data/dataset1/spiketrain-4.txt, original_data/dataset1/spiketrain-6.txt, original_data/dataset7/spiketrain-5.txt, original_data/dataset8/spiketrain-4.txt, original_data/dataset0/spiketrain-2.txt, original_data/dataset4/spiketrain-4.txt, original_data/dataset0/spiketrain-9.txt, original_data/dataset2/spiketrain-4.txt, original_data/dataset9/spiketrain-0.txt, original_data/dataset8/spiketrain-7.txt, original_data/dataset7/spiketrain-2.txt, original_data/dataset3/spiketrain-1.txt, original_data/dataset9/spiketrain-5.txt, original_data/dataset0/spiketrain-6.txt, original_data/dataset8/spiketrain-1.txt, original_data/dataset8/spiketrain-5.txt, original_data/dataset9/spiketrain-9.txt, original_data/dataset3/spiketrain-5.txt, original_data/dataset3/spiketrain-8.txt, original_data/dataset2/spiketrain-9.txt, original_data/dataset7/spiketrain-8.txt, original_data/dataset3/spiketrain-3.txt, original_data/dataset9/spiketrain-4.txt, original_data/dataset3/spiketrain-0.txt, original_data/dataset4/spiketrain-2.txt, original_data/dataset9/spiketrain-2.txt, original_data/dataset5/spiketrain-8.txt, original_data/dataset7/spiketrain-0.txt, original_data/dataset7/spiketrain-7.txt, original_data/dataset5/spiketrain-4.txt, original_data/dataset4/spiketrain-0.txt, original_data/dataset5/spiketrain-3.txt, original_data/dataset4/spiketrain-8.txt, original_data/dataset0/spiketrain-3.txt, original_data/dataset3/spiketrain-6.txt, original_data/dataset2/spiketrain-5.txt, original_data/dataset2/spiketrain-1.txt, original_data/dataset7/spiketrain-1.txt, original_data/dataset0/spiketrain-0.txt, original_data/dataset2/spiketrain-6.txt, original_data/dataset3/spiketrain-9.txt, original_data/dataset8/spiketrain-2.txt, original_data/dataset7/spiketrain-3.txt, original_data/dataset2/spiketrain-8.txt, original_data/dataset0/spiketrain-4.txt\n",
      "\n",
      "[Sun Jun  9 09:53:45 2019]\n",
      "Finished job 0.\n",
      "10 of 10 steps (100%) done\n",
      "Complete log: /home/julia/presentations/2019-06-20_Toronto/snakemake_elephant_demo/.snakemake/log/2019-06-09T095334.315253.snakemake.log\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake -pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n",
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake --dag | dot | display\n",
    "snakemake --dag | dot -Tsvg > dag5.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the generated data, we implement a utility script to load the data and a second script plot the data using matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile load_data.py\n",
    "from os.path import isdir\n",
    "import glob\n",
    "import neo\n",
    "def load_data_segment(data_location):\n",
    "    print(data_location)\n",
    "    if isdir(data_location):\n",
    "        # return assembled spiketrains from ascii files\n",
    "        segment = neo.Segment()\n",
    "        data_files = glob.glob('{}/*.txt'.format(data_location))\n",
    "        for filename in data_files:\n",
    "            io = neo.io.AsciiSpikeTrainIO(filename)\n",
    "            segment.spiketrains.extend(io.read_segment().spiketrains)\n",
    "        segment.annotate(data_sources=data_files)\n",
    "        return segment\n",
    "    else:\n",
    "        # return first segment of recording for all other supported formats\n",
    "        io = neo.io.get_io(data_location)\n",
    "        return io.read_block().segments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting plot_original_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile plot_original_data.py\n",
    "import sys\n",
    "import neo\n",
    "import matplotlib.pyplot as plt\n",
    "from load_data import load_data_segment\n",
    "def plot_original_data(data_filename, plot_filename):\n",
    "    segment = load_data_segment(data_filename)\n",
    "    for i, spiketrain in enumerate(segment.spiketrains):\n",
    "        plt.plot(spiketrain, [i]*len(spiketrain), '.')\n",
    "    plt.xlabel('Time [{}]'.format(spiketrain[0].units.dimensionality.latex))\n",
    "    plt.ylabel('Spiketrains')\n",
    "    plt.savefig(plot_filename)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    plot_original_data(*sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_data/dataset1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plot_original_data import plot_original_data\n",
    "plot_original_data('original_data/dataset1', 'original_data/dataset1/spiketrains.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Snakefile\n",
    "\n",
    "rule plot_data:\n",
    "    input: '{folder}/dataset{id}'\n",
    "    output: '{folder}/dataset{id}/spiketrains.png'\n",
    "    run: 'plot_original_data.py {input} {output}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake original_data/dataset0/spiketrains.png --dag | dot | display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we want to run an analysis on the data, which is detecting the injected higher order correlation ins the data. One of the methods suited for the detection of spatio-temporal patterns is `SPADE` [(Quaglio et al, 2017)](https://doi.org/10.3389/fncom.2017.00041)\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elephant.spade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-f\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "File extension  not registered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9027830fcc17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mrun_spade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-9027830fcc17>\u001b[0m in \u001b[0;36mrun_spade\u001b[0;34m(dataset, output_folder)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_spade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_segment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspiketrains\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# normalize spike time ranges\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mst\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/presentations/2019-06-20_Toronto/snakemake_elephant_demo/load_data.py\u001b[0m in \u001b[0;36mload_data_segment\u001b[0;34m(data_location)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# return first segment of recording for all other supported formats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/demo/lib/python3.6/site-packages/neo/io/__init__.py\u001b[0m in \u001b[0;36mget_io\u001b[0;34m(filename, *args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File extension %s not registered\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: File extension  not registered"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from load_data import load_data_segment\n",
    "import quantities as pq\n",
    "\n",
    "def run_spade(dataset, output_folder):\n",
    "    sts = load_data_segment(dataset).spiketrains\n",
    "    # normalize spike time ranges\n",
    "    for st in sts:\n",
    "        st.t_stop = max([st.t_stop for st in sts])\n",
    "    # run spade analysis\n",
    "    patterns = elephant.spade.spade(\n",
    "        data=sts, binsize=1*pq.ms, winlen=1, dither=5*pq.ms,\n",
    "        min_spikes=3, n_surr=10, psr_param=[0,0,3],\n",
    "        output_format='patterns')['patterns']\n",
    "    # save patterns\n",
    "    for i, pattern in enumerate(patterns):\n",
    "        neurons = pattern['neurons']\n",
    "        io = neo.io.AsciiSpikeTrainIO('{}/pattern-{}-neurons-{}.txt'.format(output_folder,i, '-'.join(neurons)))\n",
    "        segment = neo.Segment()\n",
    "        segment.spiketrains = [neo.SpikeTrain(pattern['times']*pq.s, t_start)]\n",
    "        io.write_segment()\n",
    "\n",
    "if __name__=='__main__':\n",
    "    run_spade(*sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-c854c5d0a45f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_pattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "plot_pattern=patterns[0]\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "# Raster plot of the data\n",
    "for st_idx, st in enumerate(sts):\n",
    "    if st_idx == 0:\n",
    "        plt.plot(st.rescale(pq.ms), [st_idx] * len(st), 'k.', label='spikes')\n",
    "    else:\n",
    "        plt.plot(st.rescale(pq.ms), [st_idx] * len(st), 'k.')\n",
    "        \n",
    "labeled=False\n",
    "for neu in plot_pattern['neurons']:\n",
    "    if labeled == False:\n",
    "        plt.plot(\n",
    "            plot_pattern['times'], [neu]*len(plot_pattern['times']), 'ro', label='pattern')\n",
    "        labeled=True\n",
    "    else:\n",
    "        plt.plot(\n",
    "            plot_pattern['times'], [neu] * len(plot_pattern['times']), 'ro')\n",
    "plt.ylim([-1, len(sts)])\n",
    "plt.xlabel('time (ms)')\n",
    "plt.ylabel('neurons ids')\n",
    "plt.yticks(range(0,100,10))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading publicly available dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Snakefile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Snakefile\n",
    "\n",
    "rule download_data:\n",
    "    output: 'original_data/dataset10/spiketrains.nev'\n",
    "    params: 'original_data/dataset10'\n",
    "    shell: 'wget -o {output} https://web.gin.g-node.org/INT/multielectrode_grasp/raw/24cd5caee3ae79066ca37844cab931d04dcad977/datasets/i140703-001.nev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building DAG of jobs...\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "snakemake original_data/dataset10/spiketrains.nev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODOs:\n",
    "* get data from gin\n",
    "* run spade on nev files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i, spiketrain in enumerate(spiketrains):\n",
    "            plt.plot(spiketrain, [i] * len(spiketrain), '.')\n",
    "plt.xlabel('Time ({})'.format(spiketrains[0].times.dimensionality.latex))\n",
    "plt.ylabel('Spiketrains')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, all spiketimes are uncorrelated, which would make this dataset a bit boring for analysis. To introduce some correlations, while keeping all other aspects constant Elephant provides the option to generate a compound poisson process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts = compound_poisson_process(rate=5*pq.Hz, A=[0]+[0.99]+[0]*9+[0.01], t_stop=10*pq.s)\n",
    "for i in range(89):\n",
    "    sts.append(homogeneous_poisson_process(rate=5*pq.Hz, t_stop=10*pq.s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Elephant* is a data analysis library built on Neo. It aims to provide a standard library for electrophysiology data analysis, merging functions from OpenElectrophy, SpykeViewer and [NeuroTools](http://neuralensemble.org/NeuroTools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOADD\n",
    "SNAKEMAKE\n",
    "* wildcards (done)\n",
    "* different rule execution statements (done)\n",
    "* conda environments\n",
    "* expand (done)\n",
    "* config\n",
    "* rulegraph\n",
    "NEO\n",
    "* class structure\n",
    "* io overview\n",
    "* intro to main data objects\n",
    "ELEPHANT\n",
    "* spade elephant demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
